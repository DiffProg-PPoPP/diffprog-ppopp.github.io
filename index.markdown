---
# Feel free to add content and custom Front Matter to this file.
# To modify the layout, see https://jekyllrb.com/docs/themes/#overriding-theme-defaults

layout: home
---

This workshop at [PPoPP'25](https://ppopp25.sigplan.org), much like its predecessor at [PPoPPâ€™24](https://ppopp24.sigplan.org/track/PPoPP-2024-workshops-and-tutorials#program), aims to bring together researchers interested in methods, tools and frameworks relying on automatic differentiation, and practitioners who need derivatives for parallel or HPC workloads, in application areas spanning applied mathematics, scientific computing, computational engineering, and machine learning. The workshop is features invited talks from both the framework developer and user communities, and solicited extended abstracts for contributed talks on topics including, but not limited to:

* Automatic differentiation tool development,
* Model, theory and method development for the differentiation of parallel computer programs,
* Differentiable languages, or domain-specific languages or frameworks that support differentiation or gradient computations,
* Case studies and experiences of computing derivatives of parallel or large scale computations, or of trying to scale differentiable applications, and
* Approaches closely related to differentiation, which may include aspects of e.g. probabilistic programming, uncertainty quantification, or error estimation.

While we encourage novel submissions, we also accept talks on previously published material. The workshop is intended as an informal venue for discussions between developers and users about ongoing or unfinished work, as well as existing methods that are not yet widely used by all relevant communities. Accepted abstracts will be shared on the workshop website, but will otherwise not lead to a formal publication.

## Schedule

| Time    | Type | Speaker | Title |
| -------- | ------- | -------------- | ------------ |
| 8:00-8:15  | Opening Words | Jan Hueckelheim | |
| 8:15-8:55 | Invited Talk | Jacques Pienaar | MLIR for ML with ML |
| 8:55-9:20 | Contributed Talk | Neil Kichler | Relaxed Differentiation and Differentiation of Relaxations: Implementation Considerations for the GPU |
| 9:20-9:45 | Contributed Talk | Alexis Montoison | Bicoloring for Automatic Sparse Differentiation |
| 9:45-10:30 | Coffee Break | | |
| 10:30-11:10 | Invited Talk | William S. Moses | EnzymeMLIR: Combining Differentiation with High-Level Optimization |
| 11:10-11:35 | Contributed Talk | Vimarsh Sathia | Compiler Optimizations for Higher-Order Automatic Differentiation |
| 11:35-12:00 | Contributed Talk | Songchen Tan | TaylorDiff.jl: Efficient and Versatile Higher-Order Derivatives in Julia |
| 12:00-12:05 | Closing Words | Jan Hueckelheim | |

## Invited Speakers

* [William S. Moses](https://wsmoses.com/academic/)
* [Jacques Pienaar](https://research.google/people/jacquespienaar/?&type=google)

## Organizers

* [Jan Hueckelheim](https://www.anl.gov/profile/jan-huckelheim)
* [Patrick Heimbach](https://www.jsg.utexas.edu/researcher/patrick_heimbach/)
* [Ludger Paehler](https://ludger.fyi)
